{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b7506e-5357-4891-b1d5-c7ead820c4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c136e16-c6cb-475d-a1ea-713d73b87d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "\n",
    "with codecs.open('cat-facts.txt', 'r', encoding='utf-8',errors='ignore') as fdata:\n",
    "\n",
    "dataset = fdata.readlines()\n",
    "print(f'Loaded {len(dataset)} entries')\n",
    "# Implement the retrieval system\n",
    "\n",
    "EMBEDDING_MODEL = 'hf.co/CompendiumLabs/bge-base-en-v1.5-gguf'\n",
    "\n",
    "LANGUAGE_MODEL = 'hf.co/bartowski/Llama-3.2-1B-Instruct-GGUF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ef07e24-cbe9-4d62-b187-eaeeede0d39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each element in the VECTOR_DB will be a tuple (chunk,embedding)\n",
    "# The embedding is a list of floats, for example: [0.1, 0.04, -0.34,0.21, ...]\n",
    "VECTOR_DB = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d145ea9-f484-48b5-a565-28e497effa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_chunk_to_database(chunk):\n",
    "    embedding = ollama.embed(model=EMBEDDING_MODEL,input=chunk)['embeddings'][0]\n",
    "    VECTOR_DB.append((chunk, embedding))\n",
    "    \n",
    "    for i, chunk in enumerate(dataset):\n",
    "        add_chunk_to_database(chunk)\n",
    "        print(f'Added chunk {i+1}/{len(dataset)} to the database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d864e0-2a88-4a88-abbf-37b90e20d45c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(a, b):\n",
    "    dot_product = sum([x * y for x, y in zip(a, b)])\n",
    "    norm_a = sum([x ** 2 for x in a]) ** 0.5\n",
    "    norm_b = sum([x ** 2 for x in b]) ** 0.5\n",
    "    return dot_product / (norm_a * norm_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4de8df-da45-495b-93ff-49d675e90fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query, top_n=3):\n",
    "    query_embedding = ollama.embed(model=EMBEDDING_MODEL,input=query)['embeddings'][0]\n",
    "    # temporary list to store (chunk, similarity) pairs\n",
    "    similarities = []\n",
    "    for chunk, embedding in VECTOR_DB:\n",
    "        similarity = cosine_similarity(query_embedding, embedding)\n",
    "        \n",
    "        similarities.append((chunk, similarity))\n",
    "        # sort by similarity in descending order, because higher similarity means more relevant chunks\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        # finally, return the top N most relevant chunks\n",
    "        return similarities[:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616c5c4c-a902-47cd-87fd-5267323d6342",
   "metadata": {},
   "source": [
    "# Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a03a38-9e42-4158-a4db-4df635d6e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_query = input('Ask me a question: ')\n",
    "retrieved_knowledge = retrieve(input_query)\n",
    "\n",
    "print('Retrieved knowledge:')\n",
    "for chunk, similarity in retrieved_knowledge:\n",
    "    print(f' - (similarity: {similarity:.2f}) {chunk}')\n",
    "    \n",
    "    instruction_prompt = f'''You are a helpful chatbot.\n",
    "    Use only the following pieces of context to answer the question.\n",
    "    Don't make up any new information:{' '.join([f' - {chunk}' for chunk, similarity in retrieved_knowledge])}'''\n",
    "# print(instruction_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c3a245-5c12-417f-b169-794beba3b85e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stream = ollama.chat( model=LANGUAGE_MODEL, messages=[{'role': 'system', 'content': instruction_prompt},{'role': 'user', 'content': input_query},],stream=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b251ee1d-0b23-4df5-b7da-7d76fdd636e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the response from the chatbot in real-time\n",
    "print('Chatbot response:')\n",
    "for chunk in stream:\n",
    "    print(chunk['message']['content'], end='', flush=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
